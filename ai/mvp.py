#!/usr/bin/env python3
# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

from time import time
import re

from langchain.prompts.chat import (
  ChatPromptTemplate,
  # SystemMessagePromptTemplate,
  # AIMessagePromptTemplate,
  # HumanMessagePromptTemplate
)
# from langchain.schema import AIMessage, HumanMessage, SystemMessage

from langchain.chat_models.base import BaseChatModel
from typing import List

import json
from os import makedirs
from os.path import split as split_path, splitext, isdir

from dotenv import load_dotenv

from ai.prompt_templates import map_template, reduce_template

import asyncio

import transcribe.transcribe_async2 as trs

'''
Minimum viable product

feeds all the transcripts to a conversational bot, asks the interview questions
'''
# TODO
# time processes
# access Amazon S3 containers
# access MongoDB
# python server, automate testing + deployment (?)

# helpful pointers when prompting:
# name/role
# system instructions
# only pull from the call
# don't make the user prompt

MODEL='gpt-4-turbo-preview'
TOKEN_MAX=128_000

# sample list and files for demo purposes
question_list = [
  'What is the workflow for UX researchers/designers when analyzing user interviews?',
  'What are the pain points for UX researchers/designers when analyzing user interviews?',
  'How have UX researchers/designers tried to solve their pain points when analyzing user interviews?',
  'What are the pain points for UX researchers when presenting user research results to other stakeholders?',
  'What are the opinions of UX researchers/designers regarding the use of AI in user research?',
]

sample_files = [
  "simple_transcripts/interview-video-1.txt",
  "simple_transcripts/interview-video-2.txt",
  "simple_transcripts/interview-video-3.txt",
]

load_dotenv()

def go(
    question_list : list[str], 
    audio_urls : dict[str, str], 
    main_dir : str = 'samples', 
    skip_transcribe : bool = False,
    skip_map : bool = False,
  ):
  '''
  Analyzes videos based on a list of questions. If you want the entire AI functionality run at once, 
  this is the function to call.
  
  :param question_list: a list of questions to answer.
  :param audio_urls: a {filename : URL_to_transcribe } dict.
  :param main_dir: the directory in which to store the result. Usually corresponds with 'username/project_name'.
  :param skip_transcribe: skips the API call to transcribe the URLs. Set to True if the files already exist in f'{main_dir}/transcripts'
  :param skip_map: skips the mapping step of the MapReduce analysis phase. Set to True if the files_response.txt already exist and are to your liking. 
  '''

  start = time()

  if not skip_transcribe:
    print("GO: transcribing urls")
    # assuming transcription redundancy is okay, or all entries into audio_urls are new
    trs.transcribe_urls(audio_urls, f'{main_dir}/transcripts')
    print("GO: transcription done.")

  print("GO: writing simple transcripts.")
  simple_trs = []

  for name in audio_urls.keys():
    filename = f'{main_dir}/transcripts/{name}.json'
    simple = simple_transcript(filename, f'{main_dir}/simple')
    simple_trs.append(simple)


  print(f'GO: calling mvp(\n\t{question_list},\n\t{simple_trs},\n\t{main_dir}/analysis)')
  mvp(question_list, simple_trs, f'{main_dir}/analysis', skip_map=skip_map)
  end = time()
  print(f'GO: done! Total time: {end - start} seconds')

  return f'{main_dir}/analysis/README.md'


# in the future: back up with prompt questions generated by examples and 
                       # another chat model call

def mvp(
    question_list : list[str] = question_list, 
    files : list[str] = sample_files, 
    response_dir : str = "mvp_response",
    skip_map : bool = False
  ):
  if not isdir(response_dir):
    makedirs(response_dir)
  
  # gather transcript
  questions : str = '\n'.join([f'{i+1}. {question_list[i]}' for i in range(len(question_list))])

  # load documents
  docs : List[Document] = []
  for filename in files:
    loader = TextLoader(filename)
    docs += loader.load()

  llm = ChatOpenAI(model=MODEL, temperature=0)
  partial_var = {'questions': questions}

  if not skip_map:
    print("MVP: we're mapping")
    time_1 = time()
    # Map
    map_chain = make_chain(llm, map_template, partial_variables=partial_var)
    map_responses = map_chain.batch(docs)
    # print responses to files
    for i in range(len(files)):
      filename = splitext(split_path(files[i])[1])[0]
      output_file = f'{response_dir}/{filename}_response.txt'
      with open(output_file, 'w') as out:
        print(map_responses[i], file=out)

    time_2 = time()
    print(f"MVP: Received all responses. Map time: {time_2 - time_1} seconds.")

  else:
    map_responses = list()
    for i in range(len(files)):
      filename = splitext(split_path(files[i])[1])[0]
      response_file = f'{response_dir}/{filename}_response.txt'
      with open(response_file, 'r') as response:
        map_responses.append(response.read())
    time_2 = time()

  print("MVP: Calling reduce.")

  # Reduce
  map_response = '\n\n'.join(map_responses)
  reduce_chain = make_chain(llm, reduce_template, partial_variables=partial_var)

  reduce_response : str = reduce_chain.invoke(map_response)

  time_3 = time()

  write_final_response(response_dir, reduce_response)

  print(f"MVP: done.\nMVP: Reduce time: {time_3 - time_2} seconds.")

  return True


def make_chain(
    llm : BaseChatModel, 
    template : str, 
    input_variables = ['docs'], 
    partial_variables : dict[str, list[str]] = {}
  ) -> LLMChain:

  prompt = PromptTemplate(
    template=template,
    input_variables=input_variables,
    partial_variables=partial_variables
  )

  chain = (
    {'docs': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
  )
  return chain


def simple_transcript(filename : str, dest_dir : str | None = None) -> str:
  '''
  creates a simple transcript from an existing complex one. 
  Stores in a .txt file. 
  Default location: "simple_transcripts/{filename}.txt"

  param filename: A deepgram .json file to get the content from.

  returns the destination file name.
  '''
  transcript = {}
  with open(filename, 'r') as f:
    transcript = json.loads(f.read())
  
  if dest_dir is None:
    dest_dir = f'simple_transcripts'

  if not isdir(dest_dir):
    makedirs(dest_dir)
  
  dest_name = f'{dest_dir}/{splitext(split_path(filename)[1])[0]}.txt'

  with open(dest_name, 'w') as f:
    print(splitext(split_path(filename)[1])[0], file=f)
    print(transcript['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript'], file=f)

  return dest_name


def write_final_response(response_dir, response):
  '''writes the final response as a README markdown'''
  with open(f'{response_dir}/README.md', 'w') as file:
    # print("# All responses", file=file)

    # Relevant quotes across interviews:(.|\n)*?(### Themes:)
    print(
      re.sub(
        r'Relevant quotes across interviews:(.|\n)*?(### Themes:)',
        r'\2',
        response
      ),
      file=file
    )


def quick_test(llm_model: str = MODEL, country : str = "Australia"):
  '''quick test to make sure model is working.'''
  prompt = ChatPromptTemplate.from_template("Hi, there! What's the capital of {country}?")
  model = ChatOpenAI(model=llm_model)
  output_parser = StrOutputParser()

  chain = {'country': RunnablePassthrough()} | prompt | model | output_parser

  response : str = chain.invoke(country)
  return response
