#!/usr/bin/env python3
# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

from time import time

from langchain.prompts.chat import (
  ChatPromptTemplate,
  # SystemMessagePromptTemplate,
  # AIMessagePromptTemplate,
  # HumanMessagePromptTemplate
)
# from langchain.schema import AIMessage, HumanMessage, SystemMessage

from langchain.chat_models.base import BaseChatModel
from typing import List

import json
import os
from dotenv import load_dotenv

from ai.prompt_templates import map_template, reduce_template

import asyncio

import transcribe.transcribe_async2 as trs

'''
Minimum viable product

feeds all the transcripts to a conversational bot, asks the interview questions
'''
# TODO
# time processes
# access Amazon S3 containers
# access MongoDB
# python server, automate testing + deployment (?)

# helpful pointers when prompting:
# name/role
# system instructions
# only pull from the call
# don't make the user prompt

MODEL='gpt-4-turbo-preview'
TOKEN_MAX=128_000

# sample list and files for demo purposes
question_list = [
  'What is the workflow for UX researchers/designers when analyzing user interviews?',
  'What are the pain points for UX researchers/designers when analyzing user interviews?',
  'How have UX researchers/designers tried to solve their pain points when analyzing user interviews?',
  'What are the pain points for UX researchers when presenting user research results to other stakeholders?',
  'What are the opinions of UX researchers/designers regarding the use of AI in user research?',
]

sample_files = [
  "simple_transcripts/interview-video-1.txt",
  "simple_transcripts/interview-video-2.txt",
  "simple_transcripts/interview-video-3.txt",
]

load_dotenv()

def quick_test(llm_model: str = MODEL, country : str = "Australia"):
  '''quick test to make sure model is working.'''
  prompt = ChatPromptTemplate.from_template("Hi, there! What's the capital of {country}?")
  model = ChatOpenAI(model=llm_model)
  output_parser = StrOutputParser()

  chain = {'country': RunnablePassthrough()} | prompt | model | output_parser

  response : str = chain.invoke(country)
  return response
  # print(chain.invoke(country))
  # return chain


def simple_transcript(filename : str, dest_dir : str | None = None) -> str:
  '''
  creates a simple transcript from an existing complex one. 
  Stores in a .txt file. 
  Default location: "simple_transcripts/{filename}.txt"

  param filename: A deepgram .json file to get the content from.

  returns the destination file name.
  '''
  transcript = {}
  with open(filename, 'r') as f:
    transcript = json.loads(f.read())
  
  if dest_dir is None:
    dest_dir = f'simple_transcripts'

  if not os.path.isdir(dest_dir):
    os.makedirs(dest_dir)
  
  dest_name = f'{dest_dir}/{os.path.splitext(os.path.split(filename)[1])[0]}.txt'

  with open(dest_name, 'w') as f:
    print(os.path.splitext(os.path.split(filename)[1])[0], file=f)
    print(transcript['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript'], file=f)

  return dest_name

# in the future: back up with prompt questions generated by examples and 
                       # another chat model call

def make_chain(llm : BaseChatModel, template : str, input_variables = ['docs'], partial_variables : dict[str, list[str]] = {}) -> LLMChain:
  prompt = PromptTemplate(
    template=template,
    input_variables=input_variables,
    partial_variables=partial_variables
  )

  chain = (
    {'docs': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
  )
  return chain


from os.path import split as split_path, splitext

def mvp(question_list : list[str] = question_list, files : list[str] = sample_files, response_dir : str = "mvp_response"):
  if not os.path.isdir(response_dir):
    os.makedirs(response_dir)
  
  # gather transcript
  questions : str = '\n'.join([f'{i+1}. {question_list[i]}' for i in range(len(question_list))])

  # load documents
  docs : List[Document] = []
  for filename in files:
    loader = TextLoader(filename)
    docs += loader.load()

  llm = ChatOpenAI(model=MODEL, temperature=0.1)
  partial_var = {'questions': questions}

  print("MVP: we're mapping")
  time_1 = time()
  # Map
  map_chain = make_chain(llm, map_template, partial_variables=partial_var)
  map_responses = map_chain.batch(docs)
  # print responses to files
  for i in range(len(files)):
    filename = splitext(split_path(files[i])[1])[0]
    output_file = f'{response_dir}/{filename}_response.txt'
    with open(output_file, 'w') as out:
      print(map_responses[i], file=out)

  time_2 = time()
  print(f"MVP: Received all responses. Map time: {time_2 - time_1} seconds.\nMVP: Calling reduce.")

  # Reduce
  map_response = '\n\n'.join(map_chain.batch(docs))
  reduce_chain = make_chain(llm, reduce_template, partial_variables=partial_var)

  reduce_response = reduce_chain.invoke(map_response)

  time_3 = time()

  with open(f'{response_dir}/README.md', 'w') as file:
    # print("# All responses", file=file)
    print(reduce_response, file=file)

  print(f"MVP: done.\nMVP: Reduce time: {time_3 - time_2} seconds.")

  return True
  

def main():
  start = time()
  skip_transcribe = True
  MAIN_DIR = 'samples'

  question_list = [
    'How do our users define the term "templates"?',
    'What feedback do people provide regarding the current gamma templates?',
    'What are people\'s expectations if they can upload their own templates to gamma?',
  ]

  # format audio urls
  audio_urls = {
    "template research interview 3":
    '4e72f777-d179-452e-8629-8ef4d76f54ad',
    "template research interview 2":
    'cb9f1ff5-178f-4c46-977d-0940fc8a9b13',
    "template research interview 4":
    '9e26f983-d3fc-438f-988d-c1c2a93bf755',
    "template research interview 5":
    '44b8d19b-531a-4947-b075-05315a2ade90',
    "template research interview 1":
    'ecdefe35-563b-4b48-8f80-f891b8303a0d',
  }
  for key in audio_urls.keys():
    audio_urls[key] = f'https://insightflow-test.s3.us-east-2.amazonaws.com/{audio_urls[key]}.mp4'

  if not skip_transcribe:
    print("MAIN: transcribing urls")
    # assuming transcription redundancy is okay, or all entries into audio_urls are new
    trs.transcribe_urls(audio_urls, f'{MAIN_DIR}/transcripts')
    print("MAIN: transcription done.")

  print("MAIN: writing simple transcripts.")
  simple_trs = []

  for name in audio_urls.keys():
    filename = f'{MAIN_DIR}/transcripts/{name}.json'
    simple = simple_transcript(filename, f'{MAIN_DIR}/simple')
    simple_trs.append(simple)


  print(f'MAIN: calling mvp(\n\t{question_list},\n\t{simple_trs},\n\t{MAIN_DIR}/analysis)')
  mvp(question_list, simple_trs, f'{MAIN_DIR}/analysis')
  end = time()
  print(f'MAIN: done! Total time: {end - start} seconds')

if __name__ == '__main__':
  main()
