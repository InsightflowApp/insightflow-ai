#!/usr/bin/env python3
# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

from langchain_core.documents import Document

from time import time
import re

from langchain.prompts.chat import (
  ChatPromptTemplate,
  # SystemMessagePromptTemplate,
  # AIMessagePromptTemplate,
  # HumanMessagePromptTemplate
)
# from langchain.schema import AIMessage, HumanMessage, SystemMessage

from langchain.chat_models.base import BaseChatModel
from typing import List

import json
from os import makedirs
from os.path import split as split_path, splitext, isdir

from dotenv import load_dotenv

from ai.prompt_templates import map_template, reduce_template

# import asyncio

import transcribe.transcribe_async as trs

'''
Minimum viable product

feeds all the transcripts to a conversational bot, asks the interview questions
'''
# TODO
# time processes
# access Amazon S3 containers
# access MongoDB
# python server, automate testing + deployment (?)

# helpful pointers when prompting:
# name/role
# system instructions
# only pull from the call
# don't make the user prompt

MODEL='gpt-4-turbo-preview'
TOKEN_MAX=128_000

load_dotenv()

def go(
    question_list : list[str], 
    audio_urls : dict[str, str], 
    main_dir : str = 'samples', 
    skip_transcribe : bool = False,
    skip_map : bool = False,
  ):
  '''
  Analyzes videos based on a list of questions. If you want the entire AI functionality run at once, 
  this is the function to call.
  
  :param question_list: a list of questions to answer.
  :param audio_urls: a {filename : URL_to_transcribe } dict.
  :param main_dir: the directory in which to store the result. Usually corresponds with 'username/project_name'.
  :param skip_transcribe: skips the API call to transcribe the URLs. Set to True if the files already exist in f'{main_dir}/transcripts'
  :param skip_map: skips the mapping step of the MapReduce analysis phase. Set to True if the files_response.txt already exist and are to your liking. 

  returns the response file name
  '''

  start = time()

  if not skip_transcribe:
    print("GO: transcribing urls")
    # assuming transcription redundancy is okay, or all entries into audio_urls are new
    trs.transcribe_urls(audio_urls, f'{main_dir}/transcripts')
    print("GO: transcription done.")

  print("GO: writing simple transcripts.")
  simple_trs = []

  for name in audio_urls.keys():
    filename = f'{main_dir}/transcripts/{name}.json'
    simple = simple_transcript(filename, f'{main_dir}/simple')
    simple_trs.append(simple)


  print(f'GO: calling mvp(\n\t{question_list},\n\t{simple_trs},\n\t{main_dir}/analysis)')
  response_filename = mvp(question_list, simple_trs, f'{main_dir}/analysis', skip_map=skip_map)
  end = time()
  print(f'GO: done! Total time: {end - start} seconds')

  return response_filename


def map_reduce(
    question_list : list[str], 
    transcripts : list[str],
  ) -> str:
  """
  maps and reduces based on input texts.
   
  returns the final response.
  """
  # gather transcript
  questions : str = '\n'.join([f'{i+1}. {question_list[i]}' for i in range(len(question_list))])

  # load documents
  docs : List[Document] = []
  docs = list(map(Document, transcripts))

  llm = ChatOpenAI(model=MODEL, temperature=0)
  partial_var = {'questions': questions}

  print("map_reduce: Calling map.")
  time_1 = time()
  # Map
  map_chain = make_chain(llm, map_template, partial_variables=partial_var)
  # these are the map responses! in case they are needed in the future
  map_responses = map_chain.batch(docs)
  time_2 = time()

  print(f"map_reduce: done mapping. Map time: {time_2 - time_1} seconds.")
  print("map_reduce: Calling reduce.")
  # Reduce
  map_response = '\n\n'.join(map_responses)
  reduce_chain = make_chain(llm, reduce_template, partial_variables=partial_var)

  reduce_response : str = reduce_chain.invoke(map_response)

  time_3 = time()

  print(f"map_reduce: done reducing. Reduce time: {time_3 - time_2} seconds.")

  return reduce_response

# in the future: back up with prompt questions generated by examples and 
                       # another chat model call

def mvp(
    question_list : list[str], 
    files : list[str],
    response_dir : str = "mvp_response",
    skip_map : bool = False
  ):
  """
  maps and reduces based on plaintext transcripts. Stores responses in a subdirectory.
   
  returns the filename of the final response.
  """
  if not isdir(response_dir):
    makedirs(response_dir)
  
  # gather transcript
  questions : str = '\n'.join([f'{i+1}. {question_list[i]}' for i in range(len(question_list))])

  # load documents
  docs : List[Document] = []
  for filename in files:
    loader = TextLoader(filename)
    docs += loader.load()

  llm = ChatOpenAI(model=MODEL, temperature=0)
  partial_var = {'questions': questions}

  if not skip_map:
    print("MVP: we're mapping")
    time_1 = time()
    # Map
    map_chain = make_chain(llm, map_template, partial_variables=partial_var)
    map_responses = map_chain.batch(docs)
    # print responses to files
    for i in range(len(files)):
      filename = splitext(split_path(files[i])[1])[0]
      output_file = f'{response_dir}/{filename}_response.txt'
      with open(output_file, 'w') as out:
        print(map_responses[i], file=out)

    time_2 = time()
    print(f"MVP: Received all responses. Map time: {time_2 - time_1} seconds.")

  else:
    map_responses = list()
    for i in range(len(files)):
      filename = splitext(split_path(files[i])[1])[0]
      response_file = f'{response_dir}/{filename}_response.txt'
      with open(response_file, 'r') as response:
        map_responses.append(response.read())
    time_2 = time()

  print("MVP: Calling reduce.")

  # Reduce
  map_response = '\n\n'.join(map_responses)
  reduce_chain = make_chain(llm, reduce_template, partial_variables=partial_var)

  reduce_response : str = reduce_chain.invoke(map_response)

  time_3 = time()

  # write response
  response_filename = write_final_response(response_dir, reduce_response)

  print(f"MVP: done.\nMVP: Reduce time: {time_3 - time_2} seconds.")

  return response_filename


def make_chain(
    llm : BaseChatModel, 
    template : str, 
    input_variables = ['docs'], 
    partial_variables : dict[str, list[str]] = {}
  ) -> LLMChain:

  prompt = PromptTemplate(
    template=template,
    input_variables=input_variables,
    partial_variables=partial_variables
  )

  chain = (
    {'docs': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
  )
  return chain


def simple_transcript(filename : str, dest_dir : str | None = None) -> str:
  '''
  creates a simple transcript from an existing complex one. 
  Stores in a .txt file. 
  Default location: "simple_transcripts/{filename}.txt"

  param filename: A deepgram .json file to get the content from.

  returns the destination file name.
  '''
  transcript = {}
  with open(filename, 'r') as f:
    transcript = json.loads(f.read())
  
  if dest_dir is None:
    dest_dir = f'simple_transcripts'

  if not isdir(dest_dir):
    makedirs(dest_dir)
  
  dest_name = f'{dest_dir}/{splitext(split_path(filename)[1])[0]}.txt'

  with open(dest_name, 'w') as f:
    print(splitext(split_path(filename)[1])[0], file=f)
    print(transcript['results']['channels'][0]['alternatives'][0]['paragraphs']['transcript'], file=f)

  return dest_name


def write_final_response(response_dir, response):
  '''writes the final response as a README markdown'''
  dest_filename = f'{response_dir}/README.md'

  with open(dest_filename, 'w') as file:
    # print("# All responses", file=file)

    # Relevant quotes across interviews:(.|\n)*?(### Themes:)
    print(
      re.sub(
        r'Relevant quotes across interviews:(.|\n)*?(### Themes:)',
        r'\2',
        response
      ),
      file=file
    )
  
  return dest_filename


def quick_test(llm_model: str = MODEL, country : str = "Australia"):
  '''quick test to make sure model is working.'''
  prompt = ChatPromptTemplate.from_template("Hi, there! What's the capital of {country}?")
  model = ChatOpenAI(model=llm_model)
  output_parser = StrOutputParser()

  chain = {'country': RunnablePassthrough()} | prompt | model | output_parser

  response : str = chain.invoke(country)
  return response
