#!/usr/bin/env python3
# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

from langchain_core.documents import Document

from time import time
import re

from langchain.prompts.chat import (
  ChatPromptTemplate,
  # SystemMessagePromptTemplate,
  # AIMessagePromptTemplate,
  # HumanMessagePromptTemplate
)
# from langchain.schema import AIMessage, HumanMessage, SystemMessage

from langchain.chat_models.base import BaseChatModel
from typing import List

from dotenv import load_dotenv

from ai.prompt_templates import map_template, reduce_template

# import asyncio

import transcribe.transcribe_async as trs

'''
Minimum viable product

feeds all the transcripts to a conversational bot, asks the interview questions
'''
# TODO
# time processes
# access Amazon S3 containers
# access MongoDB
# python server, automate testing + deployment (?)

# helpful pointers when prompting:
# name/role
# system instructions
# only pull from the call
# don't make the user prompt

MODEL='gpt-4-turbo-preview'
TOKEN_MAX=128_000

load_dotenv()


def map_reduce(
    question_list : list[str], 
    transcripts : list[str],
  ) -> str:
  """
  maps and reduces based on input texts.
   
  returns the final response.
  """
  # gather transcript
  questions : str = '\n'.join([f'{i+1}. {question_list[i]}' for i in range(len(question_list))])

  # load documents
  docs : List[Document] = []
  docs = list(map(Document, transcripts))

  llm = ChatOpenAI(model=MODEL, temperature=0)
  partial_var = {'questions': questions}

  print("map_reduce: Calling map.")
  time_1 = time()
  # Map
  map_chain = make_chain(llm, map_template, partial_variables=partial_var)
  # these are the map responses! in case they are needed in the future
  map_responses = map_chain.batch(docs)
  time_2 = time()

  print(f"map_reduce: done mapping. Map time: {time_2 - time_1} seconds.")
  print("map_reduce: Calling reduce.")
  # Reduce
  map_response = '\n\n'.join(map_responses)
  reduce_chain = make_chain(llm, reduce_template, partial_variables=partial_var)

  reduce_response : str = reduce_chain.invoke(map_response)

  time_3 = time()

  print(f"map_reduce: done reducing. Reduce time: {time_3 - time_2} seconds.")

  return reduce_response

# in the future: back up with prompt questions generated by examples and 
                       # another chat model call


def make_chain(
    llm : BaseChatModel, 
    template : str, 
    input_variables = ['docs'], 
    partial_variables : dict[str, list[str]] = {}
  ) -> LLMChain:

  prompt = PromptTemplate(
    template=template,
    input_variables=input_variables,
    partial_variables=partial_variables
  )

  chain = (
    {'docs': RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
  )
  return chain


def quick_test(llm_model: str = MODEL, country : str = "Australia"):
  '''quick test to make sure model is working.'''
  prompt = ChatPromptTemplate.from_template("Hi, there! What's the capital of {country}?")
  model = ChatOpenAI(model=llm_model)
  output_parser = StrOutputParser()

  chain = {'country': RunnablePassthrough()} | prompt | model | output_parser

  response : str = chain.invoke(country)
  return response
